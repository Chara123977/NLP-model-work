import os
import json
import numpy as np

import paddle
import matplotlib.pyplot as plt
from tqdm import tqdm
from paddlenlp.transformers import ErnieForSequenceClassification, ErnieTokenizer
from sklearn.metrics import classification_report, confusion_matrix, f1_score

# å¿½ç•¥DeprecationWarning
import warnings

warnings.filterwarnings("ignore", category=DeprecationWarning)

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—ï¼ˆè¯·ç¡®ä¿è·¯å¾„æ­£ç¡®ï¼‰
from config.chip_config import Config
from models.med_chip_mode import CHIPCTCModel
from data.med_chip_data_loader import CHIPCTCDataLoader


class CHIPCTCClassifier:
    """CHIP-CTC åŒ»ç–—æ–‡æœ¬åˆ†ç±»å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–åˆ†ç±»å™¨ï¼šåŠ è½½é…ç½®ã€è®¾ç½®matplotlibä¸­æ–‡"""
        # åŠ è½½é…ç½®
        self.config = Config()
        # åˆå§‹åŒ–æ ¸å¿ƒå±æ€§
        self.model = None
        self.tokenizer = None
        self.label2id = None
        self.id2label = None

        # matplotlib ä¸­æ–‡é…ç½®
        plt.rcParams["font.family"] = ["SimHei", "Microsoft YaHei"]
        plt.rcParams["font.size"] = 10
        plt.rcParams["axes.unicode_minus"] = False
        plt.rcParams["font.sans-serif"] = ["SimHei", "Microsoft YaHei"]

    def train(self, load_from_path=None):
        """
        å¯¹å¤–æš´éœ²çš„è®­ç»ƒæ–¹æ³•ï¼šå¯åŠ¨å®Œæ•´çš„æ¨¡å‹è®­ç»ƒæµç¨‹
        Args:
            load_from_path: ä»æŒ‡å®šè·¯å¾„åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆå¯é€‰ï¼‰
        """
        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(self.config.save_dir, exist_ok=True)

        try:
            # 1. åŠ è½½æ•°æ®
            data_loader_instance = CHIPCTCDataLoader(self.config)
            train_loader, dev_loader, test_loader = data_loader_instance.build_dataloaders()
            self.tokenizer, self.label2id, self.test_dataset = data_loader_instance.get_additional_info()
            self.id2label = {v: k for k, v in self.label2id.items()}

            # 2. æ„å»ºæ¨¡å‹
            num_classes = len(self.label2id)
            model_components = CHIPCTCModel(self.config, num_classes).build(num_classes, load_from_path)
            self.model, optimizer, loss_fn, lr_scheduler = model_components
            print(f"\nâœ… æˆåŠŸåŠ è½½æ¨¡å‹ï¼š{self.config.model_name}ï¼ˆåˆ†ç±»æ•°ï¼š{num_classes}ï¼‰")
            if load_from_path:
                print(f"âœ… ä»è·¯å¾„ç»§ç»­è®­ç»ƒï¼š{load_from_path}")

        except Exception as e:
            print(f"\nâŒ æ•°æ®/æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
            return

        # 3. å¼€å§‹è®­ç»ƒ
        print("\n========== å¼€å§‹è®­ç»ƒ ==========")
        best_macro_f1 = 0.0
        patience_counter = 0
        train_losses = []
        dev_losses = []
        dev_macro_f1s = []

        for epoch in range(self.config.epochs):
            print(f"\n===== Epoch {epoch + 1}/{self.config.epochs} =====")

            # è®­ç»ƒä¸€è½®
            train_loss = self._train_one_epoch(self.model, train_loader, loss_fn, optimizer)
            train_losses.append(train_loss)

            # æ›´æ–°å­¦ä¹ ç‡
            lr_scheduler.step()
            current_lr = lr_scheduler.get_lr()

            # éªŒè¯è¯„ä¼°
            dev_loss, dev_acc, dev_macro_f1, dev_micro_f1 = self._evaluate_model(self.model, dev_loader, loss_fn)
            dev_losses.append(dev_loss)
            dev_macro_f1s.append(dev_macro_f1)

            # æ‰“å°æœ¬è½®ç»“æœ
            print(f"è®­ç»ƒæŸå¤±ï¼š{train_loss:.4f} | å½“å‰å­¦ä¹ ç‡ï¼š{current_lr:.6f}")
            print(f"éªŒè¯æŸå¤±ï¼š{dev_loss:.4f} | éªŒè¯å‡†ç¡®ç‡ï¼š{dev_acc:.4f}")
            print(f"éªŒè¯å®F1ï¼š{dev_macro_f1:.4f} | éªŒè¯å¾®F1ï¼š{dev_micro_f1:.4f}")

            # ä¿å­˜æœ€ä¼˜æ¨¡å‹
            if dev_macro_f1 > best_macro_f1 + self.config.min_delta:
                best_macro_f1 = dev_macro_f1
                patience_counter = 0
                model_save_path = os.path.join(self.config.save_dir, "best_model")
                self.model.save_pretrained(model_save_path)
                self.tokenizer.save_pretrained(model_save_path)
                with open(os.path.join(model_save_path, "label2id.json"), "w", encoding="utf-8") as f:
                    json.dump(self.label2id, f, ensure_ascii=False, indent=2)
                print(f"âœ… ä¿å­˜æœ€ä¼˜æ¨¡å‹ï¼ˆå®F1ï¼š{best_macro_f1:.4f}ï¼‰")
            else:
                patience_counter += 1
                print(f"âš ï¸  æ—©åœè®¡æ•°å™¨ï¼š{patience_counter}/{self.config.patience}")
                if patience_counter >= self.config.patience:
                    print("âœ… æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
                    break

        # 4. è¯„ä¼°æœ€ä¼˜æ¨¡å‹
        print("\n========== è¯„ä¼°æœ€ä¼˜æ¨¡å‹ï¼ˆéªŒè¯é›†ï¼‰ ==========")
        best_model_path = os.path.join(self.config.save_dir, "best_model")
        best_model = ErnieForSequenceClassification.from_pretrained(best_model_path)
        dev_acc, dev_macro_f1, dev_micro_f1 = self._evaluate_with_report(best_model, dev_loader, loss_fn)
        print(f"\nâœ… éªŒè¯é›†æœ€ç»ˆç»“æœï¼š")
        print(f"å‡†ç¡®ç‡ï¼š{dev_acc:.4f} | å®F1ï¼š{dev_macro_f1:.4f} | å¾®F1ï¼š{dev_micro_f1:.4f}")

        # 5. ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹
        print("\n========== ç”Ÿæˆæµ‹è¯•é›†é¢„æµ‹ç»“æœ ==========")
        self._predict_test_set(best_model, test_loader, self.test_dataset)

        # 6. ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self._plot_training_curves(train_losses, dev_losses, dev_macro_f1s)

        print("\nğŸ‰ è®­ç»ƒæµç¨‹å…¨éƒ¨å®Œæˆï¼")

    def predict(self, text: str or list, model_path: str = None):
        """
        å¯¹å¤–æš´éœ²çš„é¢„æµ‹æ–¹æ³•ï¼šåŠ è½½æ¨¡å‹å¹¶é¢„æµ‹ç»™å®šæ–‡æœ¬
        Args:
            text: å•ä¸ªæ–‡æœ¬å­—ç¬¦ä¸² æˆ– æ–‡æœ¬åˆ—è¡¨
            model_path: æ¨¡å‹è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æœ€ä¼˜æ¨¡å‹è·¯å¾„
        Returns:
            é¢„æµ‹ç»“æœï¼ˆå•ä¸ªæ–‡æœ¬è¿”å›strï¼Œåˆ—è¡¨è¿”å›list[str]ï¼‰
        """
        # 1. è®¾ç½®é»˜è®¤æ¨¡å‹è·¯å¾„
        if model_path is None:
            model_path = os.path.join(self.config.save_dir, "best_model")

        # 2. åŠ è½½æ¨¡å‹å’Œtokenizerï¼ˆé¦–æ¬¡é¢„æµ‹æ—¶åŠ è½½ï¼‰
        if self.model is None or self.tokenizer is None:
            try:
                # åŠ è½½tokenizer
                self.tokenizer = ErnieTokenizer.from_pretrained(model_path)
                # åŠ è½½æ ‡ç­¾æ˜ å°„
                label2id_path = os.path.join(model_path, "label2id.json")
                with open(label2id_path, "r", encoding="utf-8") as f:
                    self.label2id = json.load(f)
                self.id2label = {v: k for k, v in self.label2id.items()}
                # åŠ è½½æ¨¡å‹
                num_classes = len(self.label2id)
                self.model = ErnieForSequenceClassification.from_pretrained(model_path, num_classes=num_classes)
                self.model.eval()
                print(f"âœ… æˆåŠŸåŠ è½½é¢„æµ‹æ¨¡å‹ï¼š{model_path}")
            except Exception as e:
                print(f"\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{e}")
                return None

        # 3. å¤„ç†è¾“å…¥æ–‡æœ¬ï¼ˆç»Ÿä¸€è½¬ä¸ºåˆ—è¡¨ï¼‰
        is_single = False
        if isinstance(text, str):
            is_single = True
            text = [text]

        # 4. æ–‡æœ¬é¢„å¤„ç† + é¢„æµ‹
        predictions = []
        with paddle.no_grad():
            for t in text:
                # æ–‡æœ¬ç¼–ç ï¼šæ˜¾å¼æ·»åŠ return_attention_mask=True
                encoded_input = self.tokenizer(
                    t,
                    max_len=self.config.max_seq_len,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pd',
                    return_attention_mask=True  # å…³é”®ä¿®å¤ï¼šå¼ºåˆ¶è¿”å›attention_mask
                )
                # æ¨¡å‹é¢„æµ‹
                logits = self.model(
                    input_ids=encoded_input['input_ids'],
                    token_type_ids=encoded_input['token_type_ids'],
                    attention_mask=encoded_input['attention_mask']
                )
                # è·å–é¢„æµ‹æ ‡ç­¾
                pred_id = paddle.argmax(logits, axis=1).item()
                pred_label = self.id2label[pred_id]
                predictions.append(pred_label)

        # 5. è¿”å›ç»“æœï¼ˆå•ä¸ªæ–‡æœ¬è¿”å›å­—ç¬¦ä¸²ï¼Œåˆ—è¡¨è¿”å›åˆ—è¡¨ï¼‰
        if is_single:
            return predictions[0]
        return predictions

    # ===================== å†…éƒ¨ç§æœ‰æ–¹æ³•ï¼ˆä¸å¯¹å¤–æš´éœ²ï¼‰ =====================
    def _train_one_epoch(self, model, train_loader, loss_fn, optimizer):
        """å•è½®è®­ç»ƒï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        model.train()
        total_loss = 0.0
        total_samples = 0
        pbar = tqdm(enumerate(train_loader), total=len(train_loader), desc="è®­ç»ƒ", ncols=100)

        for batch_idx, (input_ids, token_type_ids, attention_mask, labels) in pbar:
            logits = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
            loss = loss_fn(logits, labels)

            loss.backward()
            optimizer.step()
            optimizer.clear_grad()

            total_loss += loss.item() * len(labels)
            total_samples += len(labels)
            avg_loss = total_loss / total_samples
            pbar.set_postfix({"è®­ç»ƒæŸå¤±": f"{avg_loss:.4f}"})

        pbar.close()
        return total_loss / total_samples

    @paddle.no_grad()
    def _evaluate_model(self, model, dev_loader, loss_fn):
        """éªŒè¯æ¨¡å‹ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        model.eval()
        total_loss = 0.0
        total_correct = 0
        total_samples = 0
        all_preds = []
        all_labels = []
        pbar = tqdm(enumerate(dev_loader), total=len(dev_loader), desc="éªŒè¯", ncols=100)

        for batch_idx, batch in pbar:
            input_ids, token_type_ids, attention_mask, labels = batch
            logits = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
            loss = loss_fn(logits, labels)

            total_loss += loss.item() * len(labels)
            preds = paddle.argmax(logits, axis=1)
            total_correct += paddle.sum(preds == labels).item()
            total_samples += len(labels)

            all_preds.extend(preds.numpy().tolist())
            all_labels.extend(labels.numpy().tolist())

            avg_loss = total_loss / total_samples
            accuracy = total_correct / total_samples if total_samples > 0 else 0.0
            pbar.set_postfix({"éªŒè¯æŸå¤±": f"{avg_loss:.4f}", "å‡†ç¡®ç‡": f"{accuracy:.4f}"})

        pbar.close()
        avg_loss = total_loss / total_samples
        accuracy = total_correct / total_samples
        macro_f1 = f1_score(all_labels, all_preds, average="macro", zero_division=0)
        micro_f1 = f1_score(all_labels, all_preds, average="micro", zero_division=0)

        return avg_loss, accuracy, macro_f1, micro_f1

    @paddle.no_grad()
    def _evaluate_with_report(self, model, dev_loader, loss_fn):
        """è¯¦ç»†è¯„ä¼°ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        model.eval()
        all_preds = []
        all_labels = []
        pbar = tqdm(enumerate(dev_loader), total=len(dev_loader), desc="è¯¦ç»†è¯„ä¼°", ncols=100)

        for batch_idx, batch in pbar:
            input_ids, token_type_ids, attention_mask, labels = batch
            logits = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
            preds = paddle.argmax(logits, axis=1)
            all_preds.extend(preds.numpy().tolist())
            all_labels.extend(labels.numpy().tolist())
            pbar.set_postfix({"å·²å¤„ç†æ‰¹æ¬¡": f"{batch_idx + 1}/{len(dev_loader)}"})

        pbar.close()
        all_preds_names = [self.id2label[p] for p in all_preds]
        all_labels_names = [self.id2label[l] for l in all_labels]

        print("\n==================== éªŒè¯é›†è¯„ä¼°æŠ¥å‘Š ====================")
        print(classification_report(
            all_labels_names,
            all_preds_names,
            target_names=sorted(self.label2id.keys()),
            zero_division=0
        ))

        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(10, 8))
        cm = confusion_matrix(all_labels_names, all_preds_names, labels=sorted(self.label2id.keys()))
        plt.imshow(cm, interpolation="nearest", cmap=plt.cm.Blues)
        plt.title("CHIP-CTCå¤šåˆ†ç±»æ··æ·†çŸ©é˜µï¼ˆéªŒè¯é›†ï¼‰")
        plt.colorbar()
        plt.xlabel("é¢„æµ‹æ ‡ç­¾")
        plt.ylabel("çœŸå®æ ‡ç­¾")
        tick_marks = np.arange(len(self.label2id))
        plt.xticks(tick_marks, sorted(self.label2id.keys()), rotation=45)
        plt.yticks(tick_marks, sorted(self.label2id.keys()))

        thresh = cm.max() / 2.
        for i in range(cm.shape[0]):
            for j in range(cm.shape[1]):
                plt.text(j, i, cm[i, j], ha="center", va="center",
                         color="white" if cm[i, j] > thresh else "black")
        plt.tight_layout()
        plt.savefig("./chip_ctc_confusion_matrix.png")
        plt.show()

        accuracy = sum([1 for p, l in zip(all_preds, all_labels) if p == l]) / len(all_labels)
        macro_f1 = f1_score(all_labels, all_preds, average="macro", zero_division=0)
        micro_f1 = f1_score(all_labels, all_preds, average="micro", zero_division=0)

        return accuracy, macro_f1, micro_f1

    @paddle.no_grad()
    def _predict_test_set(self, model, test_loader, test_dataset):
        """æµ‹è¯•é›†æ‰¹é‡é¢„æµ‹ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        model.eval()
        predictions = []
        sample_idx = 0
        pbar = tqdm(enumerate(test_loader), total=len(test_loader), desc="æµ‹è¯•é›†é¢„æµ‹", ncols=100)

        for batch_idx, batch in pbar:
            input_ids, token_type_ids, attention_mask = batch
            logits = model(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)
            preds = paddle.argmax(logits, axis=1).numpy().tolist()

            batch_size = len(preds)
            batch_sample_ids = test_dataset.test_sample_ids[sample_idx: sample_idx + batch_size]
            sample_idx += batch_size

            for sample_id, pred_idx in zip(batch_sample_ids, preds):
                predictions.append({
                    "id": sample_id,
                    "label": self.id2label[pred_idx]
                })

            pbar.set_postfix({"å·²é¢„æµ‹æ ·æœ¬": f"{len(predictions)}/{len(test_dataset)}"})

        pbar.close()
        # ä¿å­˜é¢„æµ‹ç»“æœ
        with open(self.config.test_pred_save_path, "w", encoding="utf-8") as f:
            for pred in predictions:
                json.dump(pred, f, ensure_ascii=False)
                f.write("\n")
        print(f"\nâœ… æµ‹è¯•é›†é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°ï¼š{self.config.test_pred_save_path}")
        print(f"âœ… å…±é¢„æµ‹ {len(predictions)} æ¡æ— æ ‡æ³¨æ ·æœ¬")

    def _plot_training_curves(self, train_losses, dev_losses, dev_macro_f1s):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿ï¼ˆå†…éƒ¨æ–¹æ³•ï¼‰"""
        plt.switch_backend('Agg')
        plt.figure(figsize=(12, 4))

        # æŸå¤±æ›²çº¿
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, label="è®­ç»ƒæŸå¤±", marker='o', markersize=4)
        plt.plot(dev_losses, label="éªŒè¯æŸå¤±", marker='s', markersize=4)
        plt.title("è®­ç»ƒ/éªŒè¯æŸå¤±æ›²çº¿")
        plt.xlabel("Epoch")
        plt.ylabel("Loss")
        plt.legend()
        plt.grid(alpha=0.3)

        # å®F1æ›²çº¿
        plt.subplot(1, 2, 2)
        plt.plot(dev_macro_f1s, label="éªŒè¯å®F1", color="orange", marker='^', markersize=4)
        plt.title("éªŒè¯é›†å®F1æ›²çº¿")
        plt.xlabel("Epoch")
        plt.ylabel("Macro F1 Score")
        plt.legend()
        plt.grid(alpha=0.3)

        plt.tight_layout()
        curve_save_path = "./chip_ctc_training_curve.png"
        plt.savefig(curve_save_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ï¼š{curve_save_path}")


# ===================== ä½¿ç”¨ç¤ºä¾‹ =====================
if __name__ == "__main__":
    # 1. åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier = CHIPCTCClassifier()

    # 2. å¯åŠ¨è®­ç»ƒï¼ˆè°ƒç”¨å¯¹å¤–çš„trainæ–¹æ³•ï¼‰
    # classifier.train()

    # 3. ä»å·²ä¿å­˜æ¨¡å‹ç»§ç»­è®­ç»ƒï¼ˆç¤ºä¾‹ï¼‰
    # classifier.train(load_from_path="runs/best_model")

    # 4. é¢„æµ‹ç¤ºä¾‹ï¼ˆè®­ç»ƒå®Œæˆåè°ƒç”¨ï¼‰
    # å•æ–‡æœ¬é¢„æµ‹
    text = "4ï¼‰æ—¢å¾€æ‚£æœ‰éª¨é«“å¢ç”Ÿå¼‚å¸¸ç»¼åˆå¾ï¼ˆMyelodysplastic syndromeï¼ŒMDSï¼‰çš„æ‚£è€…ï¼Œæˆ–è€…æœªå®šå‹çš„æ€¥æ€§ç™½è¡€ç—…æ‚£è€…ã€‚"#"æ‚£è€…å‡ºç°å‘çƒ­ã€å’³å—½ã€èƒ¸é—·ç­‰ç—‡çŠ¶ï¼ŒæŒç»­3å¤©"
    pred_label = classifier.predict(text)
    print(f"\næ–‡æœ¬ï¼š{text}")
    print(f"é¢„æµ‹æ ‡ç­¾ï¼š{pred_label}")

    # å¤šæ–‡æœ¬é¢„æµ‹
    texts = [
        "è¡€å‹å‡é«˜ï¼Œå¤´æ™•ä¹åŠ›ï¼Œä¼´æœ‰å¿ƒæ‚¸",
        "è…¹ç—›ã€è…¹æ³»ï¼Œæ¯æ—¥5-6æ¬¡ï¼Œå¤§ä¾¿ç¨€æ°´æ ·"
    ]
    pred_labels = classifier.predict(texts)
    print(f"\nå¤šæ–‡æœ¬é¢„æµ‹ç»“æœï¼š")
    for t, l in zip(texts, pred_labels):
        print(f"æ–‡æœ¬ï¼š{t} | é¢„æµ‹æ ‡ç­¾ï¼š{l}")