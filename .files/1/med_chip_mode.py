import paddle
from paddlenlp.transformers import ErnieForSequenceClassification

class CHIPCTCModel:
    """CHIP-CTC æ–‡æœ¬åˆ†ç±»æ¨¡å‹ç±»ï¼Œå°è£…æ¨¡å‹æ„å»ºã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ç­‰é€»è¾‘"""

    def __init__(self, config, num_classes=44):
        """
        åˆå§‹åŒ–æ¨¡å‹ç±»
        Args:
            config: é…ç½®å¯¹è±¡ï¼Œéœ€åŒ…å«ä»¥ä¸‹å±æ€§ï¼š
                - model_name: é¢„è®­ç»ƒæ¨¡å‹åç§°ï¼ˆå¦‚ "ernie-3.0-base-zh"ï¼‰
                - init_lr: åˆå§‹å­¦ä¹ ç‡
                - lr_step_size: å­¦ä¹ ç‡è¡°å‡æ­¥é•¿
                - lr_gamma: å­¦ä¹ ç‡è¡°å‡ç³»æ•°
        """
        self.config = config
        self.model = None
        self.optimizer = None
        self.loss_fn = None
        self.lr_scheduler = None
        self.num_classes = None

    def build(self, num_classes):
        """
        æ„å»ºå®Œæ•´çš„æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
        Args:
            num_classes: åˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«æ•°é‡
        Returns:
            tuple: (model, optimizer, loss_fn, lr_scheduler)
        """
        self.num_classes = num_classes

        # 1. æ„å»ºERNIEåºåˆ—åˆ†ç±»æ¨¡å‹
        print(f"\nğŸ“Œ å¼€å§‹æ„å»ºæ¨¡å‹ï¼š{self.config.model_name}")
        print(f"ğŸ“Œ åˆ†ç±»ç±»åˆ«æ•°ï¼š{num_classes}")

        self.model = ErnieForSequenceClassification.from_pretrained(
            self.config.model_name,
            num_classes=num_classes,
            attention_probs_dropout_prob=0.1,  # æ³¨æ„åŠ›å±‚dropout
            hidden_dropout_prob=0.1  # éšè—å±‚dropout
        )

        # 2. æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆStepDecayï¼‰
        self.lr_scheduler = paddle.optimizer.lr.StepDecay(
            learning_rate=self.config.init_lr,
            step_size=self.config.lr_step_size,
            gamma=self.config.lr_gamma
        )

        # 3. æ„å»ºAdamWä¼˜åŒ–å™¨
        self.optimizer = paddle.optimizer.AdamW(
            learning_rate=self.lr_scheduler,
            parameters=self.model.parameters(),
            weight_decay=0.01,  # L2æ­£åˆ™åŒ–ç³»æ•°
            # å¯¹biaså’Œlayer_norm.weightä¸åº”ç”¨æƒé‡è¡°å‡
            apply_decay_param_fun=lambda x: x not in ["bias", "layer_norm.weight"]
        )

        # 4. æ„å»ºæŸå¤±å‡½æ•°ï¼ˆäº¤å‰ç†µæŸå¤±ï¼‰
        self.loss_fn = paddle.nn.CrossEntropyLoss()

        print("âœ… æ¨¡å‹ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°æ„å»ºå®Œæˆï¼")
        return self.model, self.optimizer, self.loss_fn, self.lr_scheduler

    def get_model(self):
        """è·å–æ„å»ºå¥½çš„æ¨¡å‹å®ä¾‹"""
        if self.model is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ build(num_classes) æ–¹æ³•æ„å»ºæ¨¡å‹")
        return self.model

    def get_optimizer(self):
        """è·å–æ„å»ºå¥½çš„ä¼˜åŒ–å™¨å®ä¾‹"""
        if self.optimizer is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ build(num_classes) æ–¹æ³•æ„å»ºæ¨¡å‹")
        return self.optimizer

    def get_loss_fn(self):
        """è·å–æ„å»ºå¥½çš„æŸå¤±å‡½æ•°å®ä¾‹"""
        if self.loss_fn is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ build(num_classes) æ–¹æ³•æ„å»ºæ¨¡å‹")
        return self.loss_fn

    def get_lr_scheduler(self):
        """è·å–æ„å»ºå¥½çš„å­¦ä¹ ç‡è°ƒåº¦å™¨å®ä¾‹"""
        if self.lr_scheduler is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ build(num_classes) æ–¹æ³•æ„å»ºæ¨¡å‹")
        return self.lr_scheduler

    def update_learning_rate(self):
        """æ›´æ–°å­¦ä¹ ç‡ï¼ˆè®­ç»ƒæ—¶æ¯ä¸ªepochè°ƒç”¨ï¼‰"""
        if self.lr_scheduler is None:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ build(num_classes) æ–¹æ³•æ„å»ºæ¨¡å‹")
        self.lr_scheduler.step()
        current_lr = self.lr_scheduler.get_lr()
        print(f"ğŸ”„ å­¦ä¹ ç‡å·²æ›´æ–°ï¼š{current_lr:.6f}")
        return current_lr